1.) MySQL Table (Table should have some column like created_at or updated_at so that can be used for incremental read).

Ans.
CREATE TABLE your_table_name (
    id INT NOT NULL AUTO_INCREMENT,
    column1 VARCHAR(255),
    column2 INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    PRIMARY KEY (id)
);

**************************************************************************************************************************************************************************
2.) Write a python script which is running in infinite loop and inserting 4-5 dummy/dynamically prepared records
    in MySQL Table.
Ans.
import random
import mysql.connector
from mysql.connector import Error

# MySQL connection details
db_host = 'Kanchan'
db_user = 'root'
db_password = 'root'
db_name = 'world'

# function to generate a random record
def generate_record():
    # replace with your actual column names and data types
    record = {
        'column1': random.choice(['value1', 'value2', 'value3']),
        'column2': random.randint(1, 100),
        'created_at': 'NOW()',
        'updated_at': 'NOW()'
    }
    return record

# function to insert records into MySQL table
def insert_records(records):
    try:
        connection = mysql.connector.connect(
            host=Kanchan,
            user=root,
            password=root,
            database=world
        )
        cursor = connection.cursor()
        # replace with your actual table name
        table_name = 'kafka'
        # prepare the SQL query to insert records
        query = f"INSERT INTO {table_name} (column1, column2, created_at, updated_at) VALUES (%s, %s, {records['created_at']}, {records['updated_at']})"
        # execute the query with the records
        cursor.execute(query, (records['column1'], records['column2']))
        # commit the changes to the database
        connection.commit()
        print(f"{cursor.rowcount} record(s) inserted successfully into the {table_name} table")
    except Error as e:
        print(f"Error while inserting records: {e}")
    finally:
        # close the database connection
        if connection.is_connected():
            cursor.close()
            connection.close()

# infinite loop to generate and insert records every few seconds
while True:
    records = []
    # generate 4-5 records
    for i in range(random.randint(4, 5)):
        record = generate_record()
        records.append(record)
    # insert the records into the MySQL table
    insert_records(records)
    # wait for a few seconds before generating and inserting more records
    time.sleep(random.randint(5, 10))

**************************************************************************************************************************************************************************
3.) Setup Confluent Kafka.
Ans. Setup done from following LINK

https://www.confluent.io/download/
**************************************************************************************************************************************************************************
4.) Create Topic.
Ans.
./bin/kafka-topics --create --topic My-Topic --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1

**************************************************************************************************************************************************************************
5.) Create json schema on schema registry (depends on what kind of data you are publishing in mysql table).
Ans.

CREATE TABLE users (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    email VARCHAR(100)
);

{
    "type": "object",
    "properties": {
        "id": { "type": "integer" },
        "name": { "type": "string", "maxLength": 50 },
        "email": { "type": "string", "format": "email" }
    },
    "required": ["id", "name", "email"]
}

Registering scema:

curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
    --data '{"schema": <your_json_schema>}' \
    http://localhost:8081/subjects/my-topic-value/versions

Link Schema to Kafka Topic:
{
    "schema": "<schema_id>",
    "payload": {
        "id": 1,
        "name": "John Doe",
        "email": "johndoe@example.com"
    }
}


**************************************************************************************************************************************************************************
6.) Write a producer code which will read the data from MySQL table incrementally (hint : use and maintain create_at column).
Ans.

from confluent_kafka import Producer
import mysql.connector
import json
import time

# Kafka broker configuration
bootstrap_servers = 'localhost:9092'
topic = 'my-topic'

# MySQL database configuration
mysql_host = 'Kanchan'
mysql_user = 'root'
mysql_password = 'root'
mysql_database = 'sakila'

# Create a Kafka producer instance
producer_config = {'bootstrap.servers': bootstrap_servers}
producer = Producer(producer_config)

# MySQL connection setup
mysql_conn = mysql.connector.connect(
    host=mysql_host,
    user=mysql_user,
    password=mysql_password,
    database=mysql_database
)
mysql_cursor = mysql_conn.cursor()

# Function to fetch and send data to Kafka
def send_to_kafka(data):
    try:
        producer.produce(topic, key=str(data['id']), value=json.dumps(data))
        producer.flush()
        print(f"Produced message: {data}")
    except Exception as e:
        print(f"Error producing message: {e}")

# Main loop to read and send data incrementally
def main():
    last_created_at = None

    while True:
        query = f"SELECT * FROM address WHERE created_at > '{last_created_at}' ORDER BY created_at LIMIT 100"
        mysql_cursor.execute(query)
        rows = mysql_cursor.fetchall()

        if not rows:
            print("No new data found. Sleeping for 5 seconds.")
            time.sleep(5)
            continue

        for row in rows:
            data = {
                'id': row[0],
                'name': row[1],
                'email': row[2],
                # Add other fields as needed
            }
            last_created_at = row[-1]  # Assuming created_at is the last column
            send_to_kafka(data)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Producer stopped.")
    finally:
        producer.close()
        mysql_cursor.close()
        mysql_conn.close()

**************************************************************************************************************************************************************************
8.) Write consumer group to consume data from Kafka topic.
Ans.

from confluent_kafka import Consumer, KafkaError

# Kafka broker configuration
bootstrap_servers = 'localhost:9092'
group_id = 'CONSUMER'
topics = ['my-Topic']

# Create a Kafka consumer instance
consumer_config = {
    'bootstrap.servers': bootstrap_servers,
    'group.id': group_id,
    'auto.offset.reset': 'earliest'  # Start consuming from the earliest available offset
}
consumer = Consumer(consumer_config)

# Subscribe to the topics
consumer.subscribe(topics)

# Function to process messages
def process_message(message):
    try:
        key = message.key()
        value = message.value()
        print(f"Received message: Key: {key}, Value: {value}")
    except Exception as e:
        print(f"Error processing message: {e}")

# Main loop to consume messages
def main():
    while True:
        msg = consumer.poll(1.0)  # Poll for messages every 1 second

        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                print("End of partition reached.")
            else:
                print(f"Error while consuming: {msg.error()}")
        else:
            process_message(msg)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Consumer stopped.")
    finally:
        consumer.close()

**************************************************************************************************************************************************************************
9.) In Kafka consumer code do some changes or transformation for each record and write it in Cassandra table.
Ans.

from confluent_kafka import Consumer, KafkaError
from cassandra.cluster import Cluster

# Kafka broker configuration
bootstrap_servers = 'localhost:9092'
group_id = 'CONSUMER'
topics = ['my-Topic']

# Create a Kafka consumer instance
consumer_config = {
    'bootstrap.servers': bootstrap_servers,
    'group.id': group_id,
    'auto.offset.reset': 'earliest'
}
consumer = Consumer(consumer_config)

# Cassandra configuration
cassandra_contact_points = ['localhost']
cassandra_keyspace = 'my_keyspace'

# Create a Cassandra cluster instance
cluster = Cluster(cassandra_contact_points)
session = cluster.connect(cassandra_keyspace)

# Function to process and write data to Cassandra
def process_and_write_to_cassandra(message):
    try:
        key = message.key()
        value = message.value()
        
        # Perform transformations on the value if needed
        transformed_data = {
            'id': key,
            'data': value,
            # Add other transformed fields
        }
        
        # Insert the transformed data into the Cassandra table
        insert_query = """
            INSERT INTO my_table (id, data) VALUES (?, ?)
        """
        session.execute(insert_query, (transformed_data['id'], transformed_data['data']))
        
        print(f"Processed and written to Cassandra: {transformed_data}")
    except Exception as e:
        print(f"Error processing message: {e}")

# Main loop to consume messages and write to Cassandra
def main():
    while True:
        msg = consumer.poll(1.0)

        if msg is None:
            continue
        if msg.error():
            if msg.error().code() == KafkaError._PARTITION_EOF:
                print("End of partition reached.")
            else:
                print(f"Error while consuming: {msg.error()}")
        else:
            process_and_write_to_cassandra(msg)

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("Consumer stopped.")
    finally:
        consumer.close()
        session.shutdown()
        cluster.shutdown()

**************************************************************************************************************************************************************************
