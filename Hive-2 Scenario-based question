Q1. Will it reducer work or not if you use “Limit 1” in any HiveQL query?
Ans.
When a HiveQL query includes a LIMIT 1 clause, the query engine will try to optimize the query execution by using a special operator called "Limit Operator". This operator is used to restrict the output of the query to a single row or a limited number of rows.

In some cases, the LIMIT 1 clause may allow the query engine to optimize the query execution plan by avoiding certain steps that are required for processing the entire dataset. This may include avoiding the use of reducers if the data can be processed entirely within a single mapper.

However, whether reducers will be used or not also depends on the specifics of the query and the underlying data. If the query involves complex joins, aggregations or sorting operations, reducers may still be required to process the data efficiently.

In general, the use of LIMIT 1 in a HiveQL query does not guarantee that reducers will not be used, as it depends on the specifics of the query and the underlying data.
**********************************************************************************************************************************************************************
Q2. Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans.
If multiple clients are trying to access Hive at the same time, they will connect to the same default metastore instance, which is a persistent relational database that stores metadata about Hive tables, partitions, and other objects.

By default, the metastore is implemented using Apache Derby, which is an embedded database that can only support one connection at a time. Therefore, if multiple clients try to connect to the metastore at the same time, they may experience contention and errors due to the limitation of the underlying database.

To avoid this issue, it is recommended to use a separate metastore service such as Apache MySQL or PostgreSQL, which can handle multiple concurrent connections and provide better performance and scalability. This can be configured by setting the appropriate properties in the hive-site.xml configuration file to specify the external metastore service.

Alternatively, Hive can be run in a standalone mode where the metastore is embedded within the Hive server process. This can be useful for development or testing purposes, but is not recommended for production environments due to its limited scalability and reliability.
**********************************************************************************************************************************************************************
Q3. Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans.
To optimize the performance of the query that calculates the total revenue generated for each month in the transaction_details table, you can follow these steps:

Partition the table: Partitioning the table based on the month column will allow Hive to read only the relevant data for each month, rather than scanning the entire table. To partition the table, you can use the following command:

ALTER TABLE transaction_details ADD PARTITION (month='Jan');
ALTER TABLE transaction_details ADD PARTITION (month='Feb');
...
You can create partitions for all the distinct values of the month column.

Use bucketing: Bucketing is a technique in Hive that can further improve query performance by grouping related rows together based on a hash function applied to a specific column. To bucket the table, you can use the following command:

CLUSTERED BY (cust_id) INTO 50 BUCKETS;
This command will group rows based on the cust_id column into 50 buckets.

Use the appropriate query: To calculate the total revenue generated for each month, you can use the following query:

SELECT month, SUM(amount) as total_revenue FROM transaction_details GROUP BY month;
This query will sum the amount column for each partition and bucket, and then group the results by the month column.

By partitioning the table and bucketing it based on relevant columns, Hive can optimize query execution and reduce the time taken to process the query. Additionally, you can also consider using column statistics, indexing, and tuning other configuration parameters such as memory allocation and query parallelism to further improve performance.
**********************************************************************************************************************************************************************
Q4. How can you add a new partition for the month December in the above partitioned table?
Ans.
To add a new partition for the month of December in the partitioned table transaction_details, you can use the following command:

ALTER TABLE transaction_details ADD PARTITION (month='Dec');
This command adds a new partition for the month column with the value 'Dec'. Once this command is executed, any new data that is inserted into the table with a value of 'Dec' for the month column will be stored in this new partition. You can verify the partition by running the SHOW PARTITIONS command, which will display all the partitions in the table.
**********************************************************************************************************************************************************************
Q5. I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans.
The error message "Dynamic partition strict mode requires at least one static partition column" occurs when dynamic partitioning is used without specifying any static partition columns. To resolve this error, you can either disable dynamic partitioning or add at least one static partition column.

To add a static partition column, you can modify your table definition to include the column(s) that you want to use as static partition columns. For example, if you want to use the country column as a static partition column, you can modify your table definition as follows:

CREATE TABLE transaction_details (
  cust_id INT,
  amount FLOAT,
  month STRING,
  country STRING
)
PARTITIONED BY (country)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;
This table definition includes the PARTITIONED BY clause, which specifies that the table is partitioned by the country column. This column will be used as a static partition column, while the month column can be used as a dynamic partition column.

Once you have added the static partition column, you can modify your insert statement to include the static partition column as well as the dynamic partition column:

INSERT INTO TABLE transaction_details PARTITION (month='Jan')
SELECT cust_id, amount, country FROM source_table WHERE month='Jan';
This insert statement includes both the month column as a dynamic partition column and the country column as a static partition column. You can repeat this insert statement for each month and country combination to populate the table with data.

Alternatively, if you want to disable dynamic partitioning, you can set the hive.exec.dynamic.partition.mode configuration property to nonstrict. This will allow you to insert data into partitions dynamically without specifying any static partition columns. However, keep in mind that this can lead to inefficient query performance and potential data skew issues. To set the hive.exec.dynamic.partition.mode property, you can run the following command:

SET hive.exec.dynamic.partition.mode=nonstrict;
**********************************************************************************************************************************************************************
Q6. Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans.
To consume the CSV file 'sample.csv' into the Hive warehouse using built-in SerDe, we can follow the below steps:

Create an external table in Hive with the same schema as the CSV file, using the built-in CSV SerDe. We can define the table schema as follows:

CREATE EXTERNAL TABLE sample_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  "separatorChar" = ",",
  "quoteChar" = "\""
)
LOCATION '/temp';
This creates an external table named sample_table with the schema matching the columns of the CSV file. The ROW FORMAT SERDE clause specifies that we want to use the built-in OpenCSVSerde SerDe to parse the CSV data. We also specify the separator character and quote character used in the CSV file using the SERDEPROPERTIES clause.

Load the data from the CSV file into the external table. We can use the LOAD DATA command to load the data from the CSV file into the external table as follows:

LOAD DATA LOCAL INPATH '/temp/sample.csv' INTO TABLE sample_table;
This loads the data from the local file system at /temp/sample.csv into the external table sample_table.

Verify the data has been loaded correctly. We can run a simple SELECT query to verify that the data has been loaded correctly as follows:

SELECT * FROM sample_table;
This should display all the rows from the sample_table external table.

Note: If the CSV file is present on HDFS instead of the local file system, we need to specify the HDFS path in the LOAD DATA command instead of the local file system path.
**********************************************************************************************************************************************************************
Q8. LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;


The following statement failed to execute. What can be the cause?

Q7. Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
Ans.
To solve this problem, we can use the concept of Hadoop's MapReduce jobs, which can combine multiple small files into larger ones. We can follow the below steps to create a single Hive table for lots of small files:

Combine the small CSV files into larger files using the Hadoop command - hadoop fs -getmerge input_dir output_file.

Create an external table in Hive with the required schema - {id, name, e-mail, country}.

Load the combined file into the external table using the LOAD DATA command in Hive.

The advantage of this approach is that it reduces the number of files in HDFS and improves the performance of the system. Also, we can automate the merging of small files into larger files by scheduling a job using tools like Oozie or Airflow.
**********************************************************************************************************************************************************************
Q8. LOAD DATA LOCAL INPATH ‘Home/country/state/’
The following statement failed to execute. What can be the cause?
OVERWRITE INTO TABLE address;
Ans.
The possible cause of the failure could be that the LOCAL keyword is not needed when the data is present in HDFS, and it's only used when the data is present in the local file system. Therefore, it can be corrected by removing the LOCAL keyword from the statement as shown below:

LOAD DATA INPATH '/Home/country/state/' OVERWRITE INTO TABLE address;
Also, ensure that the input path exists and has the required file permissions.
**********************************************************************************************************************************************************************
Q9. Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
Ans.
Yes, it is possible to add 100 nodes to an existing Hadoop cluster with Hive installed. We can follow the below steps to add nodes to the cluster:

Set up the hardware and install the operating system on each node.

Install Hadoop and Hive on the new nodes.

Update the configuration files of the Hadoop cluster to add the new nodes to the cluster. This involves updating the hdfs-site.xml, core-site.xml, and mapred-site.xml files with the new node information.

Restart the Hadoop services on the new nodes.

Verify that the new nodes are added to the cluster by running the jps command on each node.

Finally, run the command hadoop dfsadmin -report to check the status of the HDFS cluster, and ensure that the new nodes are successfully added.

Once the new nodes are added to the cluster, Hive can utilize the additional resources for distributed processing, which can improve the performance of Hive queries.
**********************************************************************************************************************************************************************
Q10. Hive Practical questions:
Hive Join operations
Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)
Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)
BUILD A DATA PIPELINE WITH HIVE
Ans.
Creating the tables:

First, let's create the customers table with the following columns: id, name, age, address, and salary.

CREATE TABLE customers (
  id INT,
  name STRING,
  age INT,
  address STRING,
  salary FLOAT
);
Next, let's create the orders table with the following columns: oid, date, customer_id, and amount.

CREATE TABLE orders (
  oid INT,
  date STRING,
  customer_id INT,
  amount FLOAT
);
Now, let's insert some sample data into these tables.

-- Inserting sample data into customers table
INSERT INTO customers VALUES
(1, 'Alice', 25, '123 Main St, Anytown USA', 50000),
(2, 'Bob', 30, '456 Oak Ave, Anytown USA', 60000),
(3, 'Charlie', 35, '789 Elm St, Anytown USA', 70000),
(4, 'Dave', 40, '1011 Maple Ave, Anytown USA', 80000);

-- Inserting sample data into orders table
INSERT INTO orders VALUES
(1, '2022-02-01', 1, 100),
(2, '2022-02-02', 2, 200),
(3, '2022-02-03', 1, 150),
(4, '2022-02-04', 3, 300),
(5, '2022-02-05', 1, 50);
Performing different join operations:

Inner Join:

SELECT c.name, o.oid, o.date, o.amount
FROM customers c
JOIN orders o
ON c.id = o.customer_id;
Output:


name     | oid |   date    | amount
---------|-----|-----------|-------
Alice    | 1   | 2022-02-01| 100.0
Bob      | 2   | 2022-02-02| 200.0
Alice    | 3   | 2022-02-03| 150.0
Charlie  | 4   | 2022-02-04| 300.0
Alice    | 5   | 2022-02-05| 50.0
Left Outer Join:

SELECT c.name, o.oid, o.date, o.amount
FROM customers c
LEFT JOIN orders o
ON c.id = o.customer_id;
Output:


name     | oid |   date    | amount
---------|-----|-----------|-------
Alice    | 1   | 2022-02-01| 100.0
Alice    | 3   | 2022-02-03| 150.0
Alice    | 5   | 2022-02-05| 50.0
Bob      | 2   | 2022-02-02| 200.0
Charlie  | 4   | 2022-02-04| 300.0
Dave     |     |           |
Right Outer Join:

SELECT c.name, o.oid, o.date, o.amount
FROM customers c
RIGHT JOIN orders o
ON c.id = o.customer_id;
Output:

name     | oid |   date    | amount
---------|-----|-----------|-------
Alice    | 1   | 2022-02-

** BUILD A DATA PIPELINE WITH HIVE:

To create the tables and build a data pipeline with Hive, follow the steps below:

Start by opening the Hive shell by typing hive on the terminal.
Create a database to store the tables by typing CREATE DATABASE my_db;
Use the database you just created by typing USE my_db;
Create the first table named customers by typing the following:

CREATE TABLE customers(
    id INT,
    name STRING,
    age INT,
    address STRING,
    salary FLOAT
);
Create the second table named orders by typing the following:

CREATE TABLE orders(
    oid INT,
    date STRING,
    customer_id INT,
    amount FLOAT
);
Now, let's load some data into the customers table. First, create a CSV file named customers.csv with the following data:

1,John,35,123 Main St,50000.00
2,Jane,25,456 Elm St,40000.00
3,Bob,40,789 Oak St,60000.00
Load the data from the CSV file into the customers table by typing the following:

LOAD DATA LOCAL INPATH '/path/to/customers.csv' INTO TABLE customers;
Replace /path/to/customers.csv with the actual path to the CSV file.

Next, let's load some data into the orders table. First, create a CSV file named orders.csv with the following data:

1,2022-02-01,1,1000.00
2,2022-02-02,1,500.00
3,2022-02-03,2,750.00
Load the data from the CSV file into the orders table by typing the following:

LOAD DATA LOCAL INPATH '/path/to/orders.csv' INTO TABLE orders;
Replace /path/to/orders.csv with the actual path to the CSV file.

Now, we can join the two tables on the customer_id column to get the customer's name and order amount. Type the following:

SELECT c.name, o.amount
FROM customers c
JOIN orders o ON c.id = o.customer_id;
This will return a table with the customer's name and the amount of each order they made.

Finally, let's create a view of the previous query for easier access. Type the following:

CREATE VIEW customer_orders AS
SELECT c.name, o.amount
FROM customers c
JOIN orders o ON c.id = o.customer_id;
This will create a view named customer_orders that can be accessed like a table by typing:

SELECT * FROM customer_orders;
That's it! You have successfully created two tables and built a data pipeline with Hive.
**********************************************************************************************************************************************************************
Q11. Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

*** I have taken following dataset:
****************
Dataset with electrical impedance measurements in samples of freshly excised tissue from the breast						
						
106 instances						
10 attributes: 9 features+1class attribute						
						
Six classes of freshly excised tissue were studied using electrical impedance measurements:						
				# of cases		
	Car	Carcinoma		21		
	Fad	Fibro-adenoma		15		
	Mas	Mastopathy		18		
	Gla	Glandular		16		
	Con	Connective		14		
	Adi	Adipose		22		
				106		
Impedance measurements were made at the frequencies: 15.625, 31.25, 62.5, 125, 250, 500, 1000 KHz						
These measurements plotted in the (real, -imaginary) plane constitute 						
the impedance spectrum from where the features below are computed.						
						
						
9 features:						
						
	I0	Impedivity (ohm) at zero frequency				
	PA500	phase angle at 500 KHz				
	HFS	high-frequency slope of phase angle				
	DA	impedance distance between spectral ends				
	AREA	area under spectrum				
	A/DA	area normalized by DA				
	MAX IP	maximum of the spectrum				
	DR	distance between I0 and real part of the maximum frequency point				
	P	length of the spectral curve				
						
References: 	J. Jossinet (1996) Variability of impedivity in normal and pathological breast tissue. Med. & Biol. Eng. & Comput, 34: 346-350.					
	JE Silva, JP Marques de Sá, J Jossinet (2000) Classification of Breast Tissue by Electrical Impedance Spectroscopy. Med & Bio Eng & Computing, 38:26-30.					
********************

1. Create a hive table as per given schema in your dataset 
2. try to place a data into table location
3. Perform a select operation . 
4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
7. Perform filter operation at least 5 kinds of filter examples . 
8. show and example of regex operation
9. alter table operation 
10 . drop table operation
12 . order by operation . 
13 . where clause operations you have to perform . 
14 . sorting operation you have to perform . 
15 . distinct operation you have to perform . 
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 
Ans.
Create a hive table as per given schema in your dataset

1) CREATE TABLE breast_tissue(
   I0 FLOAT,
   PA500 FLOAT,
   HFS FLOAT,
   DA FLOAT,
   AREA FLOAT,
   ADA FLOAT,
   MAXIP FLOAT,
   DR FLOAT,
   P FLOAT,
   Class STRING
);

2) We can use the following command to load the data into the breast_tissue table:
LOAD DATA INPATH '/user/hive/input/breast_tissue.csv' OVERWRITE INTO TABLE breast_tissue;

3) Perform a select operation

To select all columns from the breast_tissue table, we can use the following command:
SELECT * FROM breast_tissue;

4) Fetch the result of the select operation in your local as a CSV file
We want to save the result of the select operation in a CSV file named breast_tissue_result.csv and store it in the local file system, we can use the following command:
INSERT OVERWRITE LOCAL DIRECTORY '/path/to/local/folder' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
SELECT * FROM breast_tissue;

5) Perform group by operation
To group the data by the Class column and calculate the average value of I0 and PA500 for each class, we can use the following command:
SELECT Class, AVG(I0), AVG(PA500) FROM breast_tissue GROUP BY Class;

7) Perform filter operation at least 5 kinds of filter examples
i. To filter the data to only include rows where I0 is greater than 0.1, we can use the following command:
SELECT * FROM breast_tissue WHERE I0 > 0.1;

ii. To filter the data to only include rows where PA500 is between 0.1 and 0.5, we can use the following command:
SELECT * FROM breast_tissue WHERE PA500 BETWEEN 0.1 AND 0.5;

iii. To filter the data to only include rows where Class is either "Car" or "Mas", we can use the following command:
SELECT * FROM breast_tissue WHERE Class IN ('Car', 'Mas');

iv. To filter the data to only include rows where AREA is not null, we can use the following command:
SELECT * FROM breast_tissue WHERE AREA IS NOT NULL;

v. To filter the data to only include rows where MAXIP is null, we can use the following command:
SELECT * FROM breast_tissue WHERE MAXIP IS NULL;

8) Show an example of regex operation
To filter the data to only include rows where Class starts with "C", we can use the following command:
SELECT * FROM breast_tissue WHERE Class RLIKE '^C';

9) Alter table operation
To add a new column named VOLUME to the breast_tissue table, we can use the following command:
ALTER TABLE breast_tissue ADD COLUMNS(VOLUME FLOAT);

10) Drop table operation
To drop the breast_tissue table, we can use the following command:
DROP TABLE IF EXISTS breast_tissue;

12) Order by operation: To order the data by a particular column, we can use the "ORDER BY" clause. For example, to order the data by the "PA500" column in ascending order, we can use the following query:

SELECT * FROM breast_tissue
ORDER BY PA500 ASC;

13) Where clause operations: We can filter the data based on certain conditions using the "WHERE" clause. For example, to select only the data where the class is "Carcinoma", we can use the following query:

SELECT * FROM breast_tissue
WHERE Class = 'Car';

14) Sorting operation: To sort the data based on one or more columns, we can use the "ORDER BY" clause. For example, to sort the data first by the "Class" column in ascending order, and then by the "PA500" column in descending order, we can use the following query:

SELECT * FROM breast_tissue
ORDER BY Class ASC, PA500 DESC;
Distinct operation: To select only the unique values from a column, we can use the "DISTINCT" keyword. For example, to select only the unique values of the "Class" column, we can use the following query:

SELECT DISTINCT Class FROM breast_tissue;

15) Like operation: We can filter the data based on a pattern using the "LIKE" keyword. For example, to select only the data where the class starts with the letter "C", we can use the following query:

SELECT * FROM breast_tissue
WHERE Class LIKE 'C%';

16) Union operation: To combine the data from two or more tables, we can use the "UNION" operator. For example, to combine the data from two tables "breast_tissue_1" and "breast_tissue_2", we can use the following query:

SELECT * FROM breast_tissue_1
UNION
SELECT * FROM breast_tissue_2;

17) Table view operation: We can create a view of a table to simplify complex queries or to hide certain columns from the users. For example, to create a view that shows only the "Class" and "PA500" columns of the "breast_tissue" table, we can use the following query:

CREATE VIEW breast_tissue_view AS
SELECT Class, PA500 FROM breast_tissue;

18) We can then select data from this view just like a normal table:

SELECT * FROM breast_tissue_view;
**********************************************************************************************************************************************************************
Q12. hive operation with python:
Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations

Ans.

import pyhive
from pyhive import hive
import pandas as pd

# Create connection to Hive database
conn = hive.Connection(host="localhost", port=10000, database="your_database", auth="NOSASL")

# Create cursor object
cur = conn.cursor()

# Create a sub-table
cur.execute("CREATE TABLE sub_table AS SELECT * FROM customers WHERE age > 25")

# Drop temporary tables
cur.execute("DROP TABLE IF EXISTS temp_table")

# Fetch rows from sub-table into a Pandas DataFrame
cur.execute("SELECT * FROM sub_table")
data = pd.DataFrame(cur.fetchall(), columns=['id', 'name', 'age', 'address', 'salary'])

# Perform filter operation on DataFrame
filtered_data = data[data['salary'] > 50000]

# Fetch rows as list of tuples
cur.execute("SELECT * FROM orders")
rows = cur.fetchall()
list_of_tuples = [row for row in rows]

# Close connection
conn.close()

In this, We create a connection to a Hive database and create a sub-table by selecting data from a customers table where the age is greater than 25. We then drop a temporary table if it exists, fetch rows from the sub-table into a Pandas DataFrame, and perform a filter operation on the DataFrame. We also fetch rows from an orders table as a list of tuples. Finally, we close the connection. This code can be modified to include other join or filter operations as needed.
**********************************************************************************************************************************************************************











